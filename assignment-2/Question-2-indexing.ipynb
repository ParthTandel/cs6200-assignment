{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import os.path\n",
    "import time\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import collections\n",
    "import os\n",
    "from math import log\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(\"/home/parth/information_retrieval/Data/AP89_DATA/AP_DATA/ap89_collection/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "doc_total = 0\n",
    "for path in paths:\n",
    "    try:\n",
    "        fnames = path.split(\"/\")[-1]\n",
    "        f = open(path,\"r\", encoding='iso-8859-1')\n",
    "        text = f.read()\n",
    "        doc_total = doc_total + text.count('<DOC>')\n",
    "        for m in zip(re.finditer('<DOC>', text), re.finditer('</DOC>', text)):\n",
    "            docdata = text[m[0].start():m[1].end()]\n",
    "            file_data = {}\n",
    "            for m in zip(re.finditer('<DOCNO>', docdata), re.finditer('</DOCNO>', docdata)):\n",
    "                doc_no = docdata[m[0].end() + 1 : m[1].start()]\n",
    "                file_data['DOCNO'] = doc_no\n",
    "\n",
    "            file_data['TEXT'] = \"\"\n",
    "            for m in zip(re.finditer('<TEXT>', docdata), re.finditer('</TEXT>', docdata)):\n",
    "                file_data['TEXT'] = file_data['TEXT'] + docdata[m[0].end() + 1: m[1].start()]\n",
    "#             print(file_data)\n",
    "            data_set.append(file_data)\n",
    "    except:\n",
    "        print(path + \" error in file generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_files = open(\"stoplist.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Za-z0-9]+')\n",
    "snowball_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [snowball_stemmer.stem(stopword) for stopword in stop_files.read().split(\"\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_checker = re.compile(r\"(^[\\d][\\d+\\.]+$)\")\n",
    "numbers_checker_comma = re.compile(r\"^[\\d][\\d+\\.\\,]+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsontodocencode(jsondata, flname):\n",
    "    json_offset = {}\n",
    "    offset = 0\n",
    "    mainstring = \"\"\n",
    "    for term in jsondata.keys():\n",
    "        string = \"\"\n",
    "        string = string + str(term) + \" \" + str(jsondata[term]['ttf']) + \" \" + str(jsondata[term]['df']) + \" \"\n",
    "        for doc in jsondata[term]['tf'].keys():\n",
    "            string = string + str(doc) + \" \" + str(jsondata[term]['tf'][doc]['tf']) + \" \"\n",
    "            for pos in jsondata[term]['tf'][doc]['position']:\n",
    "                string = string + str(pos) + \" \"\n",
    "\n",
    "        \n",
    "        string = string + \"\\n\"\n",
    "        json_offset[term] = offset\n",
    "        offset = offset + len(string)\n",
    "        mainstring = mainstring + string\n",
    "\n",
    "    with open(flname + 'catlog' +'.json', 'w') as outfile:\n",
    "        json.dump(json_offset, outfile)\n",
    "    \n",
    "    with open(flname, \"w\") as output:\n",
    "        output.write(mainstring)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897.8753521442413\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "start = 0\n",
    "docid = 0\n",
    "docmap = {}\n",
    "reversedocmap = {}\n",
    "\n",
    "index = 0\n",
    "vocabmap = {}\n",
    "vocab = {}\n",
    "reverse_vocab = {}\n",
    "doc_length_dict = {}\n",
    "\n",
    "\n",
    "for i in range(1, int(len(data_set) / 1000) + 1):\n",
    "# for i in range(1,int(len(data_set_test)/2)+1):\n",
    "    fname = \"index/tokenisation_\" + str(start) + \"_\" + str(i*1000)\n",
    "    inverted_index = {}\n",
    "    file  = open(fname, \"w\")\n",
    "\n",
    "    for doc in data_set[start:(i*1000) + 1]:\n",
    "#     for doc in data_set_test[start:(i*2) + 1]:\n",
    "        docmap[doc['DOCNO']] = docid\n",
    "        reversedocmap[docid] = doc['DOCNO']\n",
    "        pos = 0\n",
    "        doc_length_dict[docid] = 0\n",
    "        for token in doc['TEXT'].split():\n",
    "            token = token.lower()\n",
    "            if(len(re.findall(numbers_checker, token)) == 1):\n",
    "                token = token\n",
    "            elif(len(re.findall(numbers_checker_comma,token)) == 1):\n",
    "                token = token.replace(\",\",\"\")\n",
    "            else:\n",
    "                token = tokenizer.tokenize(token)\n",
    "                if(len(token) > 0):\n",
    "                    token = token[0]\n",
    "                else:\n",
    "                    token = \"\"\n",
    "                token = snowball_stemmer.stem(token)\n",
    "\n",
    "            if token not in stopwords :\n",
    "                if token not in vocab.keys():\n",
    "                    vocab[token] = 0 \n",
    "                    vocabmap[token] = index\n",
    "                    reverse_vocab[index] = token\n",
    "                    index = index + 1\n",
    "\n",
    "                doc_length_dict[docid] = doc_length_dict[docid] + 1\n",
    "                token_id = vocabmap[token]\n",
    "                if token_id not in inverted_index.keys():\n",
    "                    inverted_index[token_id] = {\n",
    "                        \"ttf\" : 0,\n",
    "                        \"df\" : 0,\n",
    "                        \"tf\" : {}\n",
    "                    }\n",
    "                inverted_index[token_id][\"ttf\"] = inverted_index[token_id][\"ttf\"] + 1\n",
    "                if(docid not in inverted_index[token_id]['tf'].keys()):\n",
    "                    inverted_index[token_id]['df'] = inverted_index[token_id]['df'] + 1\n",
    "                    inverted_index[token_id]['tf'][docid] = {\n",
    "                        \"tf\" : 1,\n",
    "                        \"position\" : [pos]\n",
    "                    }\n",
    "                else:\n",
    "                    inverted_index[token_id]['tf'][docid]['tf'] =  inverted_index[token_id]['tf'][docid]['tf'] + 1\n",
    "                    inverted_index[token_id]['tf'][docid]['position'].append(pos)\n",
    "                vocab[token] = vocab[token] + 1\n",
    "                file.write(\"(\" + str(vocabmap[token]) + \" \" + str(docid) + \" \" + str(pos) + \") \" )\n",
    "                pos = pos + 1            \n",
    "        docid = docid + 1\n",
    "    \n",
    "    with open('inverted_index/' +  str(start) + \"_\" + str(i*1000) + '.json', 'w') as outfile:\n",
    "        json.dump(inverted_index, outfile)\n",
    "\n",
    "    flname = 'inverted_index_text/' +  str(start) + \"_\" + str(i*1000)\n",
    "    jsontodocencode(inverted_index, flname)\n",
    "\n",
    "    start = i * 1000 + 1\n",
    "    \n",
    "    \n",
    "#     break\n",
    "#     start = i * 2 + 1\n",
    "    \n",
    "fname = \"index/tokenisation_\" + str(start) + \"_\" + str(len(data_set))\n",
    "inverted_index = {}\n",
    "\n",
    "file  = open(fname, \"w\")\n",
    "for doc in data_set[start:]:\n",
    "    docmap[doc['DOCNO']] = docid\n",
    "    reversedocmap[docid] = doc['DOCNO']\n",
    "    doc_length_dict[docid] = 0\n",
    "    pos = 0\n",
    "    for token in doc['TEXT'].split():\n",
    "        token = token.lower()\n",
    "        if(len(re.findall(numbers_checker, token)) == 1):\n",
    "            token = token\n",
    "        elif(len(re.findall(numbers_checker_comma,token)) == 1):\n",
    "            token = token.replace(\",\",\"\")\n",
    "        else:\n",
    "            token = tokenizer.tokenize(token)\n",
    "            if(len(token) > 0):\n",
    "                token = token[0]\n",
    "            else:\n",
    "                token = \"\"\n",
    "            token = snowball_stemmer.stem(token)\n",
    "\n",
    "        if token not in stopwords :\n",
    "            if token not in vocab.keys():\n",
    "                vocab[token] = 0 \n",
    "                vocabmap[token] = index\n",
    "                reverse_vocab[index] = token\n",
    "                index = index + 1\n",
    "            \n",
    "            doc_length_dict[docid] = doc_length_dict[docid] + 1\n",
    "            token_id = vocabmap[token]\n",
    "            if token_id not in inverted_index.keys():\n",
    "                inverted_index[token_id] = {\n",
    "                    \"ttf\" : 0,\n",
    "                    \"df\" : 0,\n",
    "                    \"tf\" : {}\n",
    "                }\n",
    "            inverted_index[token_id][\"ttf\"] = inverted_index[token_id][\"ttf\"] + 1\n",
    "            if(docid not in inverted_index[token_id]['tf'].keys()):\n",
    "                inverted_index[token_id]['df'] = inverted_index[token_id]['df'] + 1\n",
    "                inverted_index[token_id]['tf'][docid] = {\n",
    "                    \"tf\" : 1,\n",
    "                    \"position\" : [pos]\n",
    "                }\n",
    "            else:\n",
    "                inverted_index[token_id]['tf'][docid]['tf'] =  inverted_index[token_id]['tf'][docid]['tf'] + 1\n",
    "                inverted_index[token_id]['tf'][docid]['position'].append(pos)\n",
    "\n",
    "            vocab[token] = vocab[token] + 1\n",
    "            file.write(\"(\" + str(vocabmap[token]) + \" \" + str(docid) + \" \" + str(pos) + \") \" )\n",
    "            pos = pos + 1\n",
    "    docid = docid + 1\n",
    "with open('inverted_index/' +  str(start) + \"_\" + str(len(data_set)) + '.json', 'w') as outfile:\n",
    "    json.dump(inverted_index, outfile)\n",
    "\n",
    "flname = 'inverted_index_text/' +  str(start) + \"_\" + str(len(data_set))\n",
    "jsontodocencode(inverted_index, flname)\n",
    "\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84678"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_length_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergefiles(fl,catlog):\n",
    "    path = \"\"\n",
    "    processed = []\n",
    "    \n",
    "    \n",
    "    path_merged = '/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/'\n",
    "\n",
    "        \n",
    "    tempfile = open(path_merged + \"temp\" , 'w')\n",
    "    \n",
    "    with open(path_merged+ 'merged.json') as f:\n",
    "        catlog_data = json.load(f)\n",
    "    \n",
    "    with open(path + catlog) as f:\n",
    "        catlog_file = json.load(f)\n",
    "\n",
    "    mergedfile = open(path_merged + 'merged', 'r')\n",
    "    file = open(path + fl, 'r')\n",
    "    \n",
    "    offset = 0\n",
    "    new_catlog = {}\n",
    "\n",
    "    \n",
    "    for term in catlog_data.keys():\n",
    "        string = \"\"\n",
    "        seek_f1 = catlog_data[term]\n",
    "        mergedfile.seek(seek_f1)\n",
    "        line_merged = mergedfile.readline().strip(\"\\n\")\n",
    "        if(term in catlog_file.keys()):\n",
    "            file.seek(catlog_file[term])\n",
    "            line_file = file.readline().strip(\"\\n\")\n",
    "            tid_ttf_df_file = line_file.split(\" \")[0:3]\n",
    "            tid_ttf_df_merged = line_merged.split(\" \")[0:3]\n",
    "            tid_ttf_df_new = str(tid_ttf_df_file[0]) + \" \" + str(int(tid_ttf_df_file[1]) + int(tid_ttf_df_merged[1]))\n",
    "            tid_ttf_df_new = tid_ttf_df_new + \" \" + str(int(tid_ttf_df_file[2]) + int(tid_ttf_df_merged[2]))\n",
    "            merged_file_pointer = len(\" \".join(tid_ttf_df_merged))\n",
    "            old_file_pointer = len(\" \".join(tid_ttf_df_file))\n",
    "            tid_ttf_df_new = tid_ttf_df_new + line_merged[merged_file_pointer:] \n",
    "            tid_ttf_df_new = tid_ttf_df_new + line_file[old_file_pointer:] + \"\\n\"\n",
    "            string = tid_ttf_df_new\n",
    "        else:\n",
    "            string = string + line_merged + \"\\n\"\n",
    "        new_catlog[term] = offset\n",
    "        offset = offset + len(string)\n",
    "        tempfile.write(string)\n",
    "        processed.append(term)\n",
    "        \n",
    "    for term in catlog_file.keys():\n",
    "        if term not in(processed):\n",
    "            processed.append(term)\n",
    "            new_catlog[term] = offset\n",
    "            file.seek(catlog_file[term])\n",
    "            string = file.readline()\n",
    "            offset = offset + len(string)\n",
    "            tempfile.write(string)\n",
    "\n",
    "    \n",
    "    mergedfile.close()\n",
    "    os.remove(path_merged + \"merged\")\n",
    "    tempfile.close()\n",
    "    os.rename(path_merged + \"temp\", path_merged + \"merged\")\n",
    "    \n",
    "    with open(path_merged + 'merged.json', 'w') as outfile:\n",
    "        json.dump(new_catlog, outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/80001_81000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/35001_36000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/82001_83000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/39001_40000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/14001_15000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/15001_16000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/3001_4000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/29001_30000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/55001_56000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/30001_31000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/4001_5000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/72001_73000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/6001_7000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/45001_46000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/10001_11000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/11001_12000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/63001_64000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/40001_41000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/0_1000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/51001_52000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/21001_22000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/54001_55000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/25001_26000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/38001_39000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/65001_66000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/76001_77000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/53001_54000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/28001_29000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/8001_9000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/32001_33000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/46001_47000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/64001_65000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/68001_69000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/56001_57000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/13001_14000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/23001_24000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/84001_84678\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/57001_58000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/49001_50000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/24001_25000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/43001_44000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/62001_63000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/67001_68000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/19001_20000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/26001_27000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/7001_8000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/16001_17000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/66001_67000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/33001_34000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/77001_78000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/42001_43000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/41001_42000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/37001_38000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/69001_70000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/75001_76000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/27001_28000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/44001_45000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/34001_35000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/20001_21000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/78001_79000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/52001_53000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/18001_19000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/59001_60000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/81001_82000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/31001_32000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/73001_74000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/47001_48000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/5001_6000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/58001_59000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/2001_3000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/61001_62000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/83001_84000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/71001_72000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/60001_61000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/9001_10000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/12001_13000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/79001_80000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/70001_71000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/50001_51000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/48001_49000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/22001_23000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/36001_37000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/1001_2000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/74001_75000\n",
      "/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/17001_18000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570.9899606704712\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "mergedPaths = glob.glob(\"/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/*\")\n",
    "for path in mergedPaths:\n",
    "    if path.find('catlog') == -1 and path.find('merged') == -1 and path.find('temp') == -1:\n",
    "        print(path)\n",
    "        mergefiles(path, path+\"catlog.json\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texttojondecode(term):\n",
    "    result = {}\n",
    "    path = \"/home/parth/information_retrieval/cs6200-assignment/assignment-2/inverted_index_text/\"\n",
    "    \n",
    "    with open(path+ 'merged.json') as f:\n",
    "        catlog_data = json.load(f)\n",
    "        \n",
    "    mergedfile = open(path + 'merged', 'r')\n",
    "\n",
    "    seek_f1 = catlog_data[str(term)]\n",
    "    mergedfile.seek(seek_f1)\n",
    "    \n",
    "    result[\"term_id\"] = term\n",
    "    result[\"term\"] = reverse_vocab[term]\n",
    " \n",
    "    txt = mergedfile.readline().strip(\"\\n\").split(\" \")\n",
    "    txt = [int(x) for x in txt if(x is not '')]\n",
    "    doc_pointer = 3\n",
    "    doc_f = int(txt[2])\n",
    "    result['doc_freq'] = doc_f\n",
    "    result['tt_freq'] = int(txt[1])\n",
    "    result['doc'] = {}\n",
    "\n",
    "    for x in range(0,doc_f):\n",
    "        result['doc'][txt[doc_pointer]] = {\n",
    "            \"tf\" : txt[doc_pointer + 1],\n",
    "            \"pos\" : []\n",
    "        } \n",
    "#         print(\"document_id:\",  txt[doc_pointer])\n",
    "#         print(\"document frequency:\",  txt[doc_pointer + 1])\n",
    "        tf = int(txt[doc_pointer + 1])\n",
    "        tf_pointer = doc_pointer + 1\n",
    "        for i in range(1, tf+1):\n",
    "            result['doc'][txt[doc_pointer]]['pos'].append(txt[tf_pointer + i])\n",
    "#             print(\"posistion  :\", txt[tf_pointer + i])\n",
    "        doc_pointer = 1 + (doc_pointer + 1) + int(txt[(doc_pointer + 1)]) \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "queriesFile = '/home/parth/information_retrieval/Data/AP89_DATA/AP_DATA/query_desc.51-100.short.txt' \n",
    "f = open(queriesFile,\"r\", encoding='iso-8859-1')\n",
    "Querytext = f.read()\n",
    "\n",
    "queries = Querytext.split('\\n')[0:-1]\n",
    "query_dict = {}\n",
    "\n",
    "for query in queries:\n",
    "    temp = query.split(\"   \")\n",
    "#     query_dict[temp[0].split(\".\")[0]] = temp[1] \n",
    "    tokens = []\n",
    "    for token in temp[1].split():\n",
    "        token = token.lower()\n",
    "        if(len(re.findall(numbers_checker, token)) == 1):\n",
    "            token = token\n",
    "        elif(len(re.findall(numbers_checker_comma,token)) == 1):\n",
    "            token = token.replace(\",\",\"\")\n",
    "        else:\n",
    "            token = tokenizer.tokenize(token)\n",
    "            if(len(token) > 0):\n",
    "                token = token[0]\n",
    "            else:\n",
    "                token = \"\"\n",
    "            token = snowball_stemmer.stem(token)\n",
    "\n",
    "        if token not in stopwords and token != 'document':\n",
    "            tokens.append(token)\n",
    "    query_dict[temp[0].split(\".\")[0]] = tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = 0 \n",
    "for doc in doc_length_dict:\n",
    "    total_length = total_length + doc_length_dict[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len = total_length/len(doc_length_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "totaldoc = len(doc_length_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_score = {}\n",
    "# for queryId in query_dict.keys():\n",
    "#     score = {}\n",
    "#     for term in query_dict[queryId]:\n",
    "#         if term in vocabmap.keys():\n",
    "#             term_id = vocabmap[term]\n",
    "#             data = texttojondecode(term_id)\n",
    "#             for doc in data['doc']:\n",
    "#                 if doc not in score.keys():\n",
    "#                     score[doc] = 0\n",
    "#                 _tf = int(data['doc'][doc]['tf']) \n",
    "#                 _l = doc_length_dict[doc]\n",
    "                \n",
    "#                 _score = _tf / (_tf + 0.5 + (1.5 * (_l/avg_len)))\n",
    "#                 score[doc] = score[doc] + _score\n",
    "#     f_score[queryId] = sorted(score.items(), key=lambda kv: kv[1])[::-1][0:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"okaptf_scores\", \"w\")\n",
    "# for query in f_score.keys():\n",
    "#     r = 1\n",
    "#     for document in f_score[query]:\n",
    "#         if(document[1] != 0):\n",
    "#             file.write(str(query) + \" Q0 \" + reversedocmap[document[0]] + \" \" + str(r) + \" \" + str(document[1]) + \" Exp\\n\" )\n",
    "#         else:\n",
    "#             print(query)\n",
    "#         r = r + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_score = {}\n",
    "# for queryId in query_dict.keys():\n",
    "#     score = {}\n",
    "#     for term in query_dict[queryId]:\n",
    "#         if term in vocabmap.keys():\n",
    "#             term_id = vocabmap[term]\n",
    "#             data = texttojondecode(term_id)\n",
    "#             for doc in data['doc']:\n",
    "#                 if doc not in score.keys():\n",
    "#                     score[doc] = 0\n",
    "#                 _tf = int(data['doc'][doc]['tf']) \n",
    "#                 _l = doc_length_dict[doc]\n",
    "                \n",
    "#                 _score = _tf / (_tf + 0.5 + (1.5 * (_l/avg_len)))\n",
    "#                 _score = _score * log(totaldoc / data['doc_freq']) \n",
    "#                 score[doc] = score[doc] + _score\n",
    "#     f_score[queryId] = sorted(score.items(), key=lambda kv: kv[1])[::-1][0:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"tfidf_scores\", \"w\")\n",
    "# for query in f_score.keys():\n",
    "#     r = 1\n",
    "#     for document in f_score[query]:\n",
    "#         if(document[1] != 0):\n",
    "#             file.write(str(query) + \" Q0 \" + reversedocmap[document[0]] + \" \" + str(r) + \" \" + str(document[1]) + \" Exp\\n\" )\n",
    "#         else:\n",
    "#             print(query)\n",
    "#         r = r + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k1 = 1.2\n",
    "# k2 = 1000\n",
    "# b = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_score = {}\n",
    "# for queryId in query_dict.keys():\n",
    "#     score = {}\n",
    "    \n",
    "#     tfq = collections.Counter(query_dict[queryId])\n",
    "    \n",
    "#     for term in query_dict[queryId]:\n",
    "#         if term in vocabmap.keys():\n",
    "#             term_id = vocabmap[term]\n",
    "#             data = texttojondecode(term_id)\n",
    "#             for doc in data['doc']:\n",
    "#                 if doc not in score.keys():\n",
    "#                     score[doc] = 0\n",
    "#                 _tf = int(data['doc'][doc]['tf']) \n",
    "#                 _l = doc_length_dict[doc]\n",
    "#                 _term_1 = (totaldoc + 0.05) / (data['doc_freq'] + 0.5)\n",
    "#                 _term_2 = ((1 + k1) * _tf) / (_tf + k1 * ((1-b) + b * ((_l/avg_len))))\n",
    "#                 _term_3 = tfq[term] * ( 1 + k2) / (tfq[term] + k2)\n",
    "#                 _score = log(_term_1) * _term_2 * _term_3 \n",
    "#                 score[doc] = score[doc] + _score\n",
    "#     f_score[queryId] = sorted(score.items(), key=lambda kv: kv[1])[::-1][0:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"okapibm25_scores\", \"w\")\n",
    "# for query in f_score.keys():\n",
    "#     r = 1\n",
    "#     for document in f_score[query]:\n",
    "#         if(document[1] != 0):\n",
    "#             file.write(str(query) + \" Q0 \" + reversedocmap[document[0]] + \" \" + str(r) + \" \" + str(document[1]) + \" Exp\\n\" )\n",
    "#         else:\n",
    "#             print(query)\n",
    "#         r = r + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V = len(reverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_score = {}\n",
    "# for queryId in query_dict.keys():\n",
    "#     doc_terms = {}\n",
    "#     score = {}\n",
    "#     tfq = collections.Counter(query_dict[queryId])\n",
    "    \n",
    "#     for term in query_dict[queryId]:\n",
    "#         if term in vocabmap.keys():\n",
    "#             term_id = vocabmap[term]\n",
    "#             data = texttojondecode(term_id)\n",
    "#             for doc in data['doc']:\n",
    "#                 if doc not in score.keys():\n",
    "#                     score[doc] = 0\n",
    "#                     doc_terms[doc] = dict(tfq)\n",
    "#                 doc_terms[doc][term] = doc_terms[doc][term] - 1\n",
    "#                 _tf = int(data['doc'][doc]['tf']) \n",
    "#                 _l = doc_length_dict[doc]\n",
    "#                 _score = log( (_tf + 1) / (_l + V))\n",
    "#                 score[doc] = score[doc] + _score\n",
    "#     for doc in doc_terms.keys():\n",
    "#         _l = doc_length_dict[doc]\n",
    "#         for word in doc_terms[doc]:\n",
    "#             score[doc] = score[doc] + (doc_terms[doc][word] * log(1 / (_l + V)))\n",
    "#     f_score[queryId] = sorted(score.items(), key=lambda kv: kv[1])[::-1][0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"laplace_scores\", \"w\")\n",
    "# for query in f_score.keys():\n",
    "#     r = 1\n",
    "#     for document in f_score[query]:\n",
    "#         if(document[1] != 0):\n",
    "#             file.write(str(query) + \" Q0 \" + reversedocmap[document[0]] + \" \" + str(r) + \" \" + str(document[1]) + \" Exp\\n\" )\n",
    "#         else:\n",
    "#             print(query)\n",
    "#         r = r + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lamda = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_score = {}\n",
    "# for queryId in query_dict.keys():\n",
    "#     doc_terms = {}\n",
    "#     score = {}\n",
    "#     tfq = collections.Counter(query_dict[queryId])\n",
    "#     ttfs = {}\n",
    "#     for term in query_dict[queryId]:\n",
    "#         if term in vocabmap.keys():\n",
    "#             term_id = vocabmap[term]\n",
    "#             data = texttojondecode(term_id)\n",
    "#             for doc in data['doc']:\n",
    "#                 if doc not in score.keys():\n",
    "#                     score[doc] = 0\n",
    "#                     doc_terms[doc] = dict(tfq)\n",
    "#                 doc_terms[doc][term] = doc_terms[doc][term] - 1\n",
    "#                 _tf = int(data['doc'][doc]['tf']) \n",
    "#                 _l = doc_length_dict[doc]\n",
    "#                 _ttf = data['tt_freq']\n",
    "#                 ttfs[term] = _ttf\n",
    "#                 _pa = (lamda * _tf) /(_l)\n",
    "#                 _pb = (1 - lamda) * (_ttf / (total_length - _l))\n",
    "#                 _score = log(_pa + _pb)\n",
    "#                 score[doc] = score[doc] + _score\n",
    "#     for doc in doc_terms.keys():\n",
    "#         _l = doc_length_dict[doc]\n",
    "#         for word in doc_terms[doc]:\n",
    "#             score[doc] = score[doc] + (doc_terms[doc][word] * log((1 - lamda) * ttfs[word] / (total_length - _l)))\n",
    "#     f_score[queryId] = sorted(score.items(), key=lambda kv: kv[1])[::-1][0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"laplace_JM_scores\", \"w\")\n",
    "# for query in f_score.keys():\n",
    "#     r = 1\n",
    "#     for document in f_score[query]:\n",
    "#         if(document[1] != 0):\n",
    "#             file.write(str(query) + \" Q0 \" + reversedocmap[document[0]] + \" \" + str(r) + \" \" + str(document[1]) + \" Exp\\n\" )\n",
    "#         else:\n",
    "#             print(query)\n",
    "#         r = r + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term_id = vocabmap['government']\n",
    "# data = texttojondecode(term_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queriesFile = 'modifiedQueries' \n",
    "# f = open(queriesFile,\"r\", encoding='iso-8859-1')\n",
    "# Querytext = f.read()\n",
    "# modifiedqueries = Querytext.split('\\n')[0:-1]\n",
    "# modified_query_dict = {}\n",
    "\n",
    "# for query in modifiedqueries:\n",
    "#     temp = query.split(\"   \")\n",
    "# #     query_dict[temp[0].split(\".\")[0]] = temp[1] \n",
    "#     tokens = []\n",
    "#     for token in temp[1].split():\n",
    "#         token = token.lower()\n",
    "#         if(len(re.findall(numbers_checker, token)) == 1):\n",
    "#             token = token\n",
    "#         elif(len(re.findall(numbers_checker_comma,token)) == 1):\n",
    "#             token = token.replace(\",\",\"\")\n",
    "#         else:\n",
    "#             token = tokenizer.tokenize(token)\n",
    "#             if(len(token) > 0):\n",
    "#                 token = token[0]\n",
    "#             else:\n",
    "#                 token = \"\"\n",
    "#             token = snowball_stemmer.stem(token)\n",
    "\n",
    "#         if token not in stopwords and token != 'document':\n",
    "#             tokens.append(token)\n",
    "#     modified_query_dict[temp[0].split(\".\")[0]] = tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findrange(test_range_dict):\n",
    "    if(len(test_range_dict) == 1) : \n",
    "        return 1000\n",
    "    sorted_pos = {}\n",
    "    sorted_pointers = {}\n",
    "    smallest_word = \"\"\n",
    "    smallest = -1\n",
    "    maxim = 0\n",
    "    for word in test_range_dict:\n",
    "        sorted_pos[word] = sorted(test_range_dict[word]['pos'])\n",
    "        sorted_pointers[word] = 0\n",
    "        if(smallest == -1 or sorted_pos[word][0] < smallest ):\n",
    "            smallest = sorted_pos[word][0]\n",
    "            smallest_word = word\n",
    "        if(sorted_pos[word][0] > maxim ):\n",
    "            maxim = sorted_pos[word][0]\n",
    "            \n",
    "\n",
    "        \n",
    "    _range =  maxim - smallest\n",
    "    \n",
    "    if(sorted_pointers[smallest_word] == len(sorted_pos[smallest_word]) - 1):\n",
    "        return _range\n",
    "    \n",
    "    sorted_pointers[smallest_word] = sorted_pointers[smallest_word] + 1\n",
    "    smallest = sorted_pos[smallest_word][sorted_pointers[smallest_word]]\n",
    "    \n",
    "    while(True):\n",
    "        for word in test_range_dict:\n",
    "            ptr = sorted_pointers[word]\n",
    "            if(sorted_pos[word][ptr] < smallest):\n",
    "                smallest = sorted_pos[word][ptr]\n",
    "                smallest_word = word\n",
    "            if(sorted_pos[word][ptr] > maxim ):\n",
    "                maxim = sorted_pos[word][ptr]\n",
    "                \n",
    "        if(sorted_pointers[smallest_word] >= len(sorted_pos[smallest_word]) - 1):\n",
    "            if((maxim - smallest) <= _range):\n",
    "                _range = maxim - smallest\n",
    "            return _range\n",
    "            \n",
    "        if((maxim - smallest) <= _range):\n",
    "            _range = maxim - smallest\n",
    "        else:\n",
    "            return _range\n",
    "        sorted_pointers[smallest_word] = sorted_pointers[smallest_word] + 1\n",
    "        smallest = sorted_pos[smallest_word][sorted_pointers[smallest_word]]\n",
    "        \n",
    "    return _range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_score = {}\n",
    "for queryId in query_dict.keys():\n",
    "    score = {}\n",
    "    doc_terms = {}\n",
    "\n",
    "    tfq = collections.Counter(query_dict[queryId])\n",
    "    \n",
    "    for term in query_dict[queryId]:\n",
    "        if term in vocabmap.keys():\n",
    "            term_id = vocabmap[term]\n",
    "            data = texttojondecode(term_id)\n",
    "            for doc in data['doc']:\n",
    "                if doc not in score.keys():\n",
    "                    score[doc] = 0\n",
    "                    doc_terms[doc] = {}\n",
    "                doc_terms[doc][term] = data['doc'][doc]\n",
    "                \n",
    "                _tf = int(data['doc'][doc]['tf']) \n",
    "                _l = doc_length_dict[doc]\n",
    "                _term_1 = (totaldoc + 0.05) / (data['doc_freq'] + 0.5)\n",
    "                _term_2 = ((1 + k1) * _tf) / (_tf + k1 * ((1-b) + b * ((_l/avg_len))))\n",
    "                _term_3 = tfq[term] * ( 1 + k2) / (tfq[term] + k2)\n",
    "                _score = log(_term_1) * _term_2 * _term_3 \n",
    "                score[doc] = score[doc] + _score\n",
    "                \n",
    "    for doc in doc_terms.keys():\n",
    "        _rangeOfWindow = findrange(doc_terms[doc])\n",
    "        _numOfContainTerms = len(doc_terms[doc])\n",
    "        _l = doc_length_dict[doc]\n",
    "        omega = _rangeOfWindow / _numOfContainTerms\n",
    "        pi = log(0.3 + exp(-omega))\n",
    "        score[doc] = score[doc] + pi\n",
    "\n",
    "    f_score[queryId] = sorted(score.items(), key=lambda kv: kv[1])[::-1][0:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"proximity_scores\", \"w\")\n",
    "# for query in f_score.keys():\n",
    "#     r = 1\n",
    "#     for document in f_score[query]:\n",
    "#         if(document[1] != 0):\n",
    "#             file.write(str(query) + \" Q0 \" + reversedocmap[document[0]] + \" \" + str(r) + \" \" + str(document[1]) + \" Exp\\n\" )\n",
    "#         else:\n",
    "#             print(query)\n",
    "#         r = r + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"proximity_scores\", \"w\")\n",
    "# for query in f_score.keys():\n",
    "#     r = 1\n",
    "#     for document in f_score[query]:\n",
    "#         if(document[1] != 0):\n",
    "#             file.write(str(query) + \" Q0 \" + reversedocmap[document[0]] + \" \" + str(r) + \" \" + str(document[1]) + \" Exp\\n\" )\n",
    "#         else:\n",
    "#             print(query)\n",
    "#         r = r + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(doc_length_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = 1500\n",
    "# for doc in doc_terms.keys():\n",
    "#     _rangeOfWindow = findrange(doc_terms[doc])\n",
    "#     _numOfContainTerms = len(doc_terms[doc])\n",
    "# #     _l = doc_length_dict[doc]\n",
    "\n",
    "#     omega = (C - _rangeOfWindow) * _numOfContainTerms / (_l + V) \n",
    "# #     print(_rangeOfWindow / , exp(-_rangeOfWindow))\n",
    "# #     break\n",
    "# #     if(len(doc_terms[doc].keys()) > 2):\n",
    "# #         print(doc_terms[doc])\n",
    "# #         print(findrange(doc_terms[doc]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
