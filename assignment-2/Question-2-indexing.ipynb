{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import os.path\n",
    "import time\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84678"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(\"/home/parth/information_retrieval/Data/AP89_DATA/AP_DATA/ap89_collection/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "doc_total = 0\n",
    "for path in paths:\n",
    "    try:\n",
    "        fnames = path.split(\"/\")[-1]\n",
    "        f = open(path,\"r\", encoding='iso-8859-1')\n",
    "        text = f.read()\n",
    "        doc_total = doc_total + text.count('<DOC>')\n",
    "        for m in zip(re.finditer('<DOC>', text), re.finditer('</DOC>', text)):\n",
    "            docdata = text[m[0].start():m[1].end()]\n",
    "            file_data = {}\n",
    "            for m in zip(re.finditer('<DOCNO>', docdata), re.finditer('</DOCNO>', docdata)):\n",
    "                doc_no = docdata[m[0].end() + 1 : m[1].start()]\n",
    "                file_data['DOCNO'] = doc_no\n",
    "\n",
    "            file_data['TEXT'] = \"\"\n",
    "            for m in zip(re.finditer('<TEXT>', docdata), re.finditer('</TEXT>', docdata)):\n",
    "                file_data['TEXT'] = file_data['TEXT'] + docdata[m[0].end() + 1: m[1].start()]\n",
    "#             print(file_data)\n",
    "            data_set.append(file_data)\n",
    "    except:\n",
    "        print(path + \" error in file generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_files = open(\"stoplist.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Za-z0-9]+')\n",
    "snowball_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [snowball_stemmer.stem(stopword) for stopword in stop_files.read().split(\"\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_checker = re.compile(r\"(^[\\d][\\d+\\.]+$)\")\n",
    "numbers_checker_comma = re.compile(r\"^[\\d][\\d+\\.\\,]+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "docid = 0\n",
    "docmap = {}\n",
    "reversedocmap = {}\n",
    "\n",
    "index = 0\n",
    "vocabmap = {}\n",
    "vocab = {}\n",
    "reverse_vocab = {}\n",
    "\n",
    "for i in range(1, int(len(data_set) / 1000) + 1):\n",
    "    fname = \"index/tokenisation_\" + str(start) + \"_\" + str(i*1000)\n",
    "#     print(fname)\n",
    "    file  = open(fname, \"w\")\n",
    "\n",
    "    for doc in data_set[start:(i*1000) + 1]:\n",
    "        docmap[doc['DOCNO']] = docid\n",
    "        reversedocmap[docid] = doc['DOCNO']\n",
    "        pos = 0\n",
    "        for token in doc['TEXT'].split():\n",
    "            token = token.lower()\n",
    "            if(len(re.findall(numbers_checker, token)) == 1):\n",
    "                token = token\n",
    "            elif(len(re.findall(numbers_checker_comma,token)) == 1):\n",
    "                token = token.replace(\",\",\"\")\n",
    "            else:\n",
    "                token = tokenizer.tokenize(token)\n",
    "                if(len(token) > 0):\n",
    "                    token = token[0]\n",
    "                else:\n",
    "                    token = \"\"\n",
    "                token = snowball_stemmer.stem(token)\n",
    "\n",
    "            if token not in stopwords :\n",
    "                if token not in vocab.keys():\n",
    "                    vocab[token] = 0 \n",
    "                    vocabmap[token] = index\n",
    "                    index = index + 1\n",
    "                    reverse_vocab[index] = token\n",
    "\n",
    "                vocab[token] = vocab[token] + 1\n",
    "                pos = pos + 1\n",
    "                file.write(\"(\" + str(vocabmap[token]) + \" \" + str(docid) + \" \" + str(pos) + \") \" )\n",
    "\n",
    "        docid = docid + 1\n",
    "    start = i * 1000 + 1\n",
    "    \n",
    "fname = \"index/tokenisation_\" + str(start) + \"_\" + str(len(data_set))\n",
    "file  = open(fname, \"w\")\n",
    "for doc in data_set[start:]:\n",
    "    docmap[doc['DOCNO']] = docid\n",
    "    reversedocmap[docid] = doc['DOCNO']\n",
    "    pos = 0\n",
    "    for token in doc['TEXT'].split():\n",
    "        token = token.lower()\n",
    "        if(len(re.findall(numbers_checker, token)) == 1):\n",
    "            token = token\n",
    "        elif(len(re.findall(numbers_checker_comma,token)) == 1):\n",
    "            token = token.replace(\",\",\"\")\n",
    "        else:\n",
    "            token = tokenizer.tokenize(token)\n",
    "            if(len(token) > 0):\n",
    "                token = token[0]\n",
    "            else:\n",
    "                token = \"\"\n",
    "            token = snowball_stemmer.stem(token)\n",
    "\n",
    "        if token not in stopwords :\n",
    "            if token not in vocab.keys():\n",
    "                vocab[token] = 0 \n",
    "                vocabmap[token] = index\n",
    "                index = index + 1\n",
    "                reverse_vocab[index] = token\n",
    "\n",
    "            vocab[token] = vocab[token] + 1\n",
    "            pos = pos + 1\n",
    "            file.write(\"(\" + str(vocabmap[token]) + \" \" + str(docid) + \" \" + str(pos) + \") \" )\n",
    "\n",
    "    docid = docid + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file  = open(\"tokenisation\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n"
     ]
    }
   ],
   "source": [
    "for token in tokens.split(\") \"):\n",
    "    splits = token.strip(\"(\").split(\" \")\n",
    "    try:\n",
    "        if(int(splits[1]) == (x * 1000 + 1)):\n",
    "            print(x, end=\" \")\n",
    "            x = x + 1\n",
    "    except:\n",
    "        print(splits)\n",
    "#     else:\n",
    "#         print(splits, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1000\n",
      "1001 2000\n",
      "2001 3000\n",
      "3001 4000\n",
      "4001 5000\n",
      "5001 6000\n",
      "6001 7000\n",
      "7001 8000\n",
      "8001 9000\n",
      "9001 10000\n",
      "10001 11000\n",
      "11001 12000\n",
      "12001 13000\n",
      "13001 14000\n",
      "14001 15000\n",
      "15001 16000\n",
      "16001 17000\n",
      "17001 18000\n",
      "18001 19000\n",
      "19001 20000\n",
      "20001 21000\n",
      "21001 22000\n",
      "22001 23000\n",
      "23001 24000\n",
      "24001 25000\n",
      "25001 26000\n",
      "26001 27000\n",
      "27001 28000\n",
      "28001 29000\n",
      "29001 30000\n",
      "30001 31000\n",
      "31001 32000\n",
      "32001 33000\n",
      "33001 34000\n",
      "34001 35000\n",
      "35001 36000\n",
      "36001 37000\n",
      "37001 38000\n",
      "38001 39000\n",
      "39001 40000\n",
      "40001 41000\n",
      "41001 42000\n",
      "42001 43000\n",
      "43001 44000\n",
      "44001 45000\n",
      "45001 46000\n",
      "46001 47000\n",
      "47001 48000\n",
      "48001 49000\n",
      "49001 50000\n",
      "50001 51000\n",
      "51001 52000\n",
      "52001 53000\n",
      "53001 54000\n",
      "54001 55000\n",
      "55001 56000\n",
      "56001 57000\n",
      "57001 58000\n",
      "58001 59000\n",
      "59001 60000\n",
      "60001 61000\n",
      "61001 62000\n",
      "62001 63000\n",
      "63001 64000\n",
      "64001 65000\n",
      "65001 66000\n",
      "66001 67000\n",
      "67001 68000\n",
      "68001 69000\n",
      "69001 70000\n",
      "70001 71000\n",
      "71001 72000\n",
      "72001 73000\n",
      "73001 74000\n",
      "74001 75000\n",
      "75001 76000\n",
      "76001 77000\n",
      "77001 78000\n",
      "78001 79000\n",
      "79001 80000\n",
      "80001 81000\n",
      "81001 82000\n",
      "82001 83000\n",
      "83001 84000\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "start = 1\n",
    "for i in range(84600):\n",
    "    if(i == (x * 1000 + 1)):\n",
    "        print (start , x*1000)\n",
    "        start = x * 1000 + 1\n",
    "        x = x + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
