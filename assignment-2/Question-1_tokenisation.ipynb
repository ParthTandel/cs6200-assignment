{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import os.path\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(\"/home/parth/information_retrieval/Data/AP89_DATA/AP_DATA/ap89_collection/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "doc_total = 0\n",
    "for path in paths:\n",
    "    try:\n",
    "        fnames = path.split(\"/\")[-1]\n",
    "        f = open(path,\"r\", encoding='iso-8859-1')\n",
    "        text = f.read()\n",
    "        doc_total = doc_total + text.count('<DOC>')\n",
    "        for m in zip(re.finditer('<DOC>', text), re.finditer('</DOC>', text)):\n",
    "            docdata = text[m[0].start():m[1].end()]\n",
    "            file_data = {}\n",
    "            for m in zip(re.finditer('<DOCNO>', docdata), re.finditer('</DOCNO>', docdata)):\n",
    "                doc_no = docdata[m[0].end() + 1 : m[1].start()]\n",
    "                file_data['DOCNO'] = doc_no\n",
    "\n",
    "            file_data['TEXT'] = \"\"\n",
    "            for m in zip(re.finditer('<TEXT>', docdata), re.finditer('</TEXT>', docdata)):\n",
    "                file_data['TEXT'] = file_data['TEXT'] + docdata[m[0].end() + 1: m[1].start()]\n",
    "#             print(file_data)\n",
    "            data_set.append(file_data)\n",
    "    except:\n",
    "        print(path + \" error in file generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_files = open(\"stoplist.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [snowball_stemmer.stem(stopword) for stopword in stop_files.read().split(\"\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers_checker = re.compile(r\"[0-9]+\\.[0-9]+|[0-9]+\")\n",
    "# numbers_checker = re.compile(r\"([0-9+\\.]+$)\")\n",
    "# patern = re.compile(r'?\\d+(?:,\\d*)?')\n",
    "# pattern.match(\"123,12\").start()\n",
    "# \"123.134\".count(r\"[0-9]+.[0-9]+\")\n",
    "# numbers_checker_comma = re.compile(r\"([0-9]+,[0-9]+\\.[0-9]+|[0-9]+,[0-9]+)\")\n",
    "# numbers_checker = re.compile(r\"(\\d{1,0})\")\n",
    "# len(re.findall(pattern, \"12312\"))\n",
    "# re.findall(numbers_checker, \"5110.\")\n",
    "\n",
    "numbers_checker = re.compile(r\"(^[\\d][\\d+\\.]+$)\")\n",
    "numbers_checker_comma = re.compile(r\"^[\\d][\\d+\\.\\,]+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649.0925583839417\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[A-Za-z0-9]+') // \n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "docid = 0\n",
    "docmap = {}\n",
    "reversedocmap = {}\n",
    "vocabmap = {}\n",
    "index = 0\n",
    "reverse_vocab = {}\n",
    "\n",
    "file  = open(\"tokenisation\", \"w\")\n",
    "start = time.time()\n",
    "for doc in data_set:\n",
    "    pos = 0\n",
    "    docmap[doc['DOCNO']] = docid\n",
    "    reversedocmap[docid] = doc['DOCNO']\n",
    "    for token in doc['TEXT'].split():\n",
    "        token = token.lower()\n",
    "        if(len(re.findall(numbers_checker, token)) == 1):\n",
    "            token = token\n",
    "        elif(len(re.findall(numbers_checker_comma,token)) == 1):\n",
    "            token = token.replace(\",\",\"\")\n",
    "        else:\n",
    "            # token = snowball_stemmer.stem(re.sub(r\"[\\.]|[,]|[']|[\\\"]|[_]|[`]\", \"\", token))\n",
    "            token = tokenizer.tokenize(token)\n",
    "            if(len(token) > 0):\n",
    "                token = token[0]\n",
    "            else:\n",
    "                token = \"\"\n",
    "            token = snowball_stemmer.stem(token)\n",
    "\n",
    "        if token not in stopwords :\n",
    "            if token not in vocab.keys():\n",
    "                vocab[token] = 0 \n",
    "                vocabmap[token] = index\n",
    "                index = index + 1\n",
    "                reverse_vocab[index] = token\n",
    "\n",
    "            vocab[token] = vocab[token] + 1\n",
    "            pos = pos + 1\n",
    "    #         print(\"(\" + str(vocabmap[token]) + \" \" + str(docid) + \" \" + str(pos) + \") \" , end=', ' )\n",
    "            file.write(\"(\" + str(vocabmap[token]) + \" \" + str(docid) + \" \" + str(pos) + \") \" )\n",
    "\n",
    "    docid = docid + 1\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['northwestern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3897"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabmap['maximum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'northwestern'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_vocab[295]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for key in vocab.keys():\n",
    "    cnt = cnt + vocab[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20242236"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.json', 'w') as outfile:\n",
    "    json.dump(vocab, outfile)\n",
    "    \n",
    "with open('docmap.json', 'w') as outfile:\n",
    "    json.dump(docmap, outfile)\n",
    "    \n",
    "with open('reversedocmap.json', 'w') as outfile:\n",
    "    json.dump(reversedocmap, outfile)\n",
    "    \n",
    "with open('vocabmap.json', 'w') as outfile:\n",
    "    json.dump(vocabmap, outfile)\n",
    "\n",
    "    \n",
    "with open('reverse_vocab.json', 'w') as outfile:\n",
    "    json.dump(vocabmap, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"0 144 105 84068 1 275 84229 1 336 84012 1 23 84490 1 63 84491 3 25 94 132 84631 1 157 84237 1 177 84242 1 8 84500 1 18 84503 2 62 83 84257 1 137 84258 1 137 84059 1 150 84004 2 80 339 84262 1 274 84572 1 52 84268 1 119 84278 1 213 84279 2 49 112 84280 2 172 261 84025 1 250 84191 1 117 84030 1 265 84031 1 304 84036 2 150 246 84662 2 26 79 84038 1 61 84551 3 8 21 84 84040 1 187 84555 1 121 84045 1 46 84047 5 53 72 162 218 246 84560 1 102 84051 1 195 84564 1 115 84054 1 168 84281 3 27 89 150 84056 1 168 84324 1 355 84315 1 136 84316 1 82 84666 2 40 56 84318 1 28 84578 1 59 84310 1 193 84438 1 219 84586 1 250 84075 1 308 84079 1 146 84592 5 2 29 145 209 222 84627 2 172 186 84286 2 318 338 84087 2 337 342 84600 1 597 84090 1 199 84092 2 11 16 84605 1 47 84607 1 46 84358 1 162 84615 1 129 84247 1 119 84370 2 63 67 84371 1 249 84116 1 79 84630 1 61 84119 3 23 48 81 84633 1 206 84122 1 72 84381 1 44 84126 1 44 84385 1 241 84387 1 52 84645 1 50 84648 2 17 69 84653 2 31 194 84655 3 3 50 144 84168 2 103 232 84406 1 160 84151 1 359 84154 1 400 84156 1 291 84157 1 312 84674 1 60 84163 1 103 84420 2 14 88 84214 1 152 84424 1 165 84172 2 385 418 84176 1 63 84434 2 146 241 84435 1 90 84182 2 141 343 84188 1 328 84190 1 437 84447 1 218 84192 1 214 84454 2 475 479 84455 1 204 84462 1 267 84209 1 160 84210 1 90 84212 1 47 84470 2 13 190 84217 1 140 84221 1 42 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test', 'w') as outfile:\n",
    "    outfile.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "['1', '2', '1', '0', '2', '2', '3']\n",
      "[1, 2, 1, 0, 2, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "file = open('/home/parth/information_retrieval/cs6200-assignment/assignment-2/testfiles/test_file_1', 'r')\n",
    "file.seek(14)\n",
    "txt = file.readline()\n",
    "print(len(txt))\n",
    "txt = txt.replace(\"\\n\", \"\").split(\" \")\n",
    "print(txt)\n",
    "txt = [ int(x) for x in txt]\n",
    "print(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file.seek(49279)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 0, 2, 2, 3]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document_id: 0\n",
      "document frequency: 2\n",
      "posistion  : 2\n",
      "posistion  : 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_pointer = 3\n",
    "doc_f = int(txt[2])\n",
    "for x in range(0,doc_f):\n",
    "    print(\"document_id:\",  txt[doc_pointer])\n",
    "    print(\"document frequency:\",  txt[doc_pointer + 1])\n",
    "    tf = int(txt[doc_pointer + 1])\n",
    "    tf_pointer = doc_pointer + 1\n",
    "    for i in range(1, tf+1):\n",
    "        print(\"posistion  :\", txt[tf_pointer + i])\n",
    "    doc_pointer = 1 + (doc_pointer + 1) + int(txt[(doc_pointer + 1)]) \n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
