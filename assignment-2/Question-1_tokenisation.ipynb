{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import os.path\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(\"/home/parth/information_retrieval/Data/AP89_DATA/AP_DATA/ap89_collection/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "doc_total = 0\n",
    "for path in paths:\n",
    "    try:\n",
    "        fnames = path.split(\"/\")[-1]\n",
    "        f = open(path,\"r\", encoding='iso-8859-1')\n",
    "        text = f.read()\n",
    "        doc_total = doc_total + text.count('<DOC>')\n",
    "        for m in zip(re.finditer('<DOC>', text), re.finditer('</DOC>', text)):\n",
    "            docdata = text[m[0].start():m[1].end()]\n",
    "            file_data = {}\n",
    "            for m in zip(re.finditer('<DOCNO>', docdata), re.finditer('</DOCNO>', docdata)):\n",
    "                doc_no = docdata[m[0].end() + 1 : m[1].start()]\n",
    "                file_data['DOCNO'] = doc_no\n",
    "\n",
    "            file_data['TEXT'] = \"\"\n",
    "            for m in zip(re.finditer('<TEXT>', docdata), re.finditer('</TEXT>', docdata)):\n",
    "                file_data['TEXT'] = file_data['TEXT'] + docdata[m[0].end() + 1: m[1].start()]\n",
    "#             print(file_data)\n",
    "            data_set.append(file_data)\n",
    "    except:\n",
    "        print(path + \" error in file generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_files = open(\"stoplist.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [snowball_stemmer.stem(stopword) for stopword in stop_files.read().split(\"\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers_checker = re.compile(r\"[0-9]+\\.[0-9]+|[0-9]+\")\n",
    "# numbers_checker = re.compile(r\"([0-9+\\.]+$)\")\n",
    "# patern = re.compile(r'?\\d+(?:,\\d*)?')\n",
    "# pattern.match(\"123,12\").start()\n",
    "# \"123.134\".count(r\"[0-9]+.[0-9]+\")\n",
    "# numbers_checker_comma = re.compile(r\"([0-9]+,[0-9]+\\.[0-9]+|[0-9]+,[0-9]+)\")\n",
    "# numbers_checker = re.compile(r\"(\\d{1,0})\")\n",
    "# len(re.findall(pattern, \"12312\"))\n",
    "# re.findall(numbers_checker, \"5110.\")\n",
    "\n",
    "numbers_checker = re.compile(r\"(^[\\d][\\d+\\.]+$)\")\n",
    "numbers_checker_comma = re.compile(r\"^[\\d][\\d+\\.\\,]+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649.0925583839417\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[A-Za-z0-9]+')\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "docid = 0\n",
    "docmap = {}\n",
    "reversedocmap = {}\n",
    "vocabmap = {}\n",
    "index = 0\n",
    "reverse_vocab = {}\n",
    "\n",
    "file  = open(\"tokenisation\", \"w\")\n",
    "start = time.time()\n",
    "for doc in data_set:\n",
    "    pos = 0\n",
    "    docmap[doc['DOCNO']] = docid\n",
    "    reversedocmap[docid] = doc['DOCNO']\n",
    "    for token in doc['TEXT'].split():\n",
    "        token = token.lower()\n",
    "        if(len(re.findall(numbers_checker, token)) == 1):\n",
    "            token = token\n",
    "        elif(len(re.findall(numbers_checker_comma,token)) == 1):\n",
    "            token = token.replace(\",\",\"\")\n",
    "        else:\n",
    "            # token = snowball_stemmer.stem(re.sub(r\"[\\.]|[,]|[']|[\\\"]|[_]|[`]\", \"\", token))\n",
    "            token = tokenizer.tokenize(token)\n",
    "            if(len(token) > 0):\n",
    "                token = token[0]\n",
    "            else:\n",
    "                token = \"\"\n",
    "            token = snowball_stemmer.stem(token)\n",
    "\n",
    "        if token not in stopwords :\n",
    "            if token not in vocab.keys():\n",
    "                vocab[token] = 0 \n",
    "                vocabmap[token] = index\n",
    "                index = index + 1\n",
    "                reverse_vocab[index] = token\n",
    "\n",
    "            vocab[token] = vocab[token] + 1\n",
    "            pos = pos + 1\n",
    "    #         print(\"(\" + str(vocabmap[token]) + \" \" + str(docid) + \" \" + str(pos) + \") \" , end=', ' )\n",
    "            file.write(\"(\" + str(vocabmap[token]) + \" \" + str(docid) + \" \" + str(pos) + \") \" )\n",
    "\n",
    "    docid = docid + 1\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['northwestern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3897"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabmap['maximum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'northwestern'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_vocab[295]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for key in vocab.keys():\n",
    "    cnt = cnt + vocab[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20242236"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.json', 'w') as outfile:\n",
    "    json.dump(vocab, outfile)\n",
    "    \n",
    "with open('docmap.json', 'w') as outfile:\n",
    "    json.dump(docmap, outfile)\n",
    "    \n",
    "with open('reversedocmap.json', 'w') as outfile:\n",
    "    json.dump(reversedocmap, outfile)\n",
    "    \n",
    "with open('vocabmap.json', 'w') as outfile:\n",
    "    json.dump(vocabmap, outfile)\n",
    "\n",
    "    \n",
    "with open('reverse_vocab.json', 'w') as outfile:\n",
    "    json.dump(vocabmap, outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
