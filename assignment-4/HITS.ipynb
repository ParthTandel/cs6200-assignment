{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from scipy import sparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "import elasticsearch_dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled = {}\n",
    "results = elasticsearch.helpers.scan(es,\n",
    "    index=\"crawled_data_merged\",\n",
    "    doc_type=\"doc\",\n",
    "    preserve_order=True,\n",
    "    query={\"query\": {\"match_all\": {}}},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results:\n",
    "    crawled[item['_id']] = item['_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 12000 13000 14000 15000 16000 17000 18000 19000 20000 21000 22000 23000 24000 25000 26000 27000 28000 29000 30000 31000 32000 33000 34000 35000 36000 37000 38000 "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "graph = {}\n",
    "\n",
    "for link in crawled:\n",
    "    if link not in graph:\n",
    "        graph[link] = {\n",
    "            \"inlinks\" : [],\n",
    "            \"outlinks\": []\n",
    "        }\n",
    "\n",
    "    outlink = []\n",
    "    \n",
    "    for ol in crawled[link][\"OUTLINK\"]:\n",
    "        if ol in crawled:\n",
    "            outlink.append(ol)\n",
    "        else:\n",
    "            o = urlparse(ol)\n",
    "            outlink.append(o.scheme + \"://\" + o.netloc)\n",
    "    graph[link][\"outlinks\"] = list(set(outlink))\n",
    "    \n",
    "    \n",
    "    for in_l in graph[link][\"outlinks\"]:\n",
    "        if in_l not in graph:\n",
    "            graph[in_l] = {\n",
    "                \"inlinks\" : [],\n",
    "                \"outlinks\": []\n",
    "            }           \n",
    "        graph[in_l][\"inlinks\"].append(link)\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(i,end =\" \")\n",
    "    i = i + 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in graph:\n",
    "    graph[node][\"inlinks\"] = list(set(graph[node][\"inlinks\"]))\n",
    "    graph[node][\"outlinks\"] = list(set(graph[node][\"outlinks\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ittr = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HITS(term, ittr, eps):\n",
    "#     root_set = []\n",
    "#     base_set = []\n",
    "\n",
    "    \n",
    "    body = {\n",
    "        \"query\": {\n",
    "                \"match\": {\n",
    "                  \"TEXT\": term        \n",
    "                }\n",
    "            },\n",
    "        \"size\" : 1000\n",
    "    }\n",
    "\n",
    "    terms = es.search(index = \"crawled_data_merged\",doc_type=\"doc\", body=body)['hits']['hits']\n",
    "\n",
    "    root_set = {}\n",
    "    base_set = {}\n",
    "\n",
    "    \n",
    "    for link in terms:\n",
    "        root_set[link['_id']] = True\n",
    "        base_set[link['_id']] = True\n",
    "        \n",
    "    itr = 0\n",
    "    \n",
    "    \n",
    "    while(len(set(root_set)) <= 10000):\n",
    "        for link in root_set:\n",
    "#             if link.find(\"quotes.liberty-tree.ca\") != -1:\n",
    "#                 continue\n",
    "\n",
    "            _set = []\n",
    "            _set = _set + graph[link][\"inlinks\"]\n",
    "            _set = _set + graph[link][\"outlinks\"]\n",
    "            _set = list(set(_set))\n",
    "            \n",
    "            if len(_set) > 200:\n",
    "                _set = np.array(_set)\n",
    "                _set = _set[random.sample(range(0, len(_set)), 200)]\n",
    "                _set = _set.tolist()\n",
    "            \n",
    "            for _url in _set:\n",
    "                base_set[_url] = True\n",
    "            \n",
    "            if(len(base_set) > 10000 and itr != 0) :\n",
    "                break\n",
    "\n",
    "        for link in base_set:\n",
    "#             if link.find(\"quotes.liberty-tree.ca\") != -1:\n",
    "#                 continue\n",
    "            root_set[link] = True\n",
    "        itr += 1\n",
    "        \n",
    "        if(itr == 5):\n",
    "            break\n",
    "        print(itr, end=\", \")\n",
    "\n",
    "        \n",
    "    base_set_mapping = {}\n",
    "    base_rev_set_mapping = {}\n",
    "\n",
    "    id_bs = 0\n",
    "    for url in root_set:\n",
    "        base_set_mapping[url] = id_bs\n",
    "        base_rev_set_mapping[id_bs] = url\n",
    "        id_bs += 1\n",
    "        \n",
    "    adjcency = np.zeros((len(base_set), len(base_set)))\n",
    "    \n",
    "    st = time.time()\n",
    "    for link in base_set:\n",
    "        _id = base_set_mapping[link]\n",
    "\n",
    "        for out in graph[link]['outlinks']:\n",
    "\n",
    "            if out not in base_set_mapping:\n",
    "                continue\n",
    "\n",
    "            _out_id = base_set_mapping[out]\n",
    "            if _id == _out_id:\n",
    "                continue\n",
    "            adjcency[_id][_out_id] = 1\n",
    "\n",
    "        for ins in graph[link]['inlinks']:\n",
    "\n",
    "            if ins not in base_set_mapping:\n",
    "                continue\n",
    "\n",
    "\n",
    "            _in_id = base_set_mapping[ins]\n",
    "            if _id == _in_id:\n",
    "                continue\n",
    "            adjcency[_out_id][_id] = 1\n",
    "    \n",
    "    print(time.time() - st)\n",
    "        \n",
    "    hubs = np.ones(len(base_set))\n",
    "    auth = np.ones(len(base_set)) #transpose matrix\n",
    "    \n",
    "    auth = auth / math.sqrt(sum(np.square(auth)))\n",
    "    hubs = hubs / math.sqrt(sum(np.square(hubs)))\n",
    "    \n",
    "    for i in range(ittr):\n",
    "    \n",
    "        auth_new = adjcency.T.dot(hubs)\n",
    "        hubs_new = adjcency.dot(auth)\n",
    "\n",
    "        auth_new = auth_new / math.sqrt(sum(np.square(auth_new)))\n",
    "        hubs_new = hubs_new / math.sqrt(sum(np.square(hubs_new)))\n",
    "\n",
    "        authdiff = np.abs(auth_new - auth)\n",
    "        hubdiff = np.abs(hubs_new - hubs)\n",
    "\n",
    "        if (authdiff > eps).any():\n",
    "            auth = auth_new\n",
    "            hubs = hubs_new\n",
    "            continue\n",
    "        else:    \n",
    "            print (i, \"found\")\n",
    "            break    \n",
    "    \n",
    "    with open(term.replace(\" \", \"_\") +\"_authority\",\"w\") as file:\n",
    "        for id in auth.argsort()[::-1][0:500]:\n",
    "            file.write(base_rev_set_mapping[id]+\"    \"+ str(auth[id]) +\"\\n\")\n",
    "            \n",
    "    with open(term.replace(\" \", \"_\") +\"_hub\",\"w\") as file:\n",
    "        for id in hubs.argsort()[::-1][0:500]:\n",
    "            file.write(base_rev_set_mapping[id]+\"    \"+ str(hubs[id])+\"\\n\")    \n",
    "\n",
    "    return root_set, base_set_mapping, base_rev_set_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 0.4417448043823242\n",
      "38 found\n"
     ]
    }
   ],
   "source": [
    "base_set, base_set_mapping, base_rev_set_mapping = HITS(\"american revolutionary war\", 1000, eps = 1.0e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(base_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjcency = np.zeros((len(base_set), len(base_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st = time.time()\n",
    "# for link in base_set:\n",
    "#     _id = base_set_mapping[link]\n",
    "    \n",
    "#     for out in graph[link]['outlinks']:\n",
    "        \n",
    "#         if out not in base_set_mapping:\n",
    "#             continue\n",
    "\n",
    "#         _out_id = base_set_mapping[out]\n",
    "#         if _id == _out_id:\n",
    "#             continue\n",
    "#         adjcency[_id][_out_id] = 1\n",
    "        \n",
    "#     for ins in graph[link]['inlinks']:\n",
    "        \n",
    "#         if ins not in base_set_mapping:\n",
    "#             continue\n",
    "\n",
    "        \n",
    "#         _in_id = base_set_mapping[ins]\n",
    "#         if _id == _in_id:\n",
    "#             continue\n",
    "#         adjcency[_out_id][_id] = 1\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "# print(time.time() - st)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max(sum(adjcency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hubs = np.ones(len(base_set))\n",
    "# auth = np.ones(len(base_set)) #transpose matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth = auth / math.sqrt(sum(np.square(auth)))\n",
    "# hubs = hubs / math.sqrt(sum(np.square(hubs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps = 1.0e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "    \n",
    "#     auth_new = adjcency.T.dot(hubs)\n",
    "#     hubs_new = adjcency.dot(auth)\n",
    "    \n",
    "#     auth_new = auth_new / math.sqrt(sum(np.square(auth_new)))\n",
    "#     hubs_new = hubs_new / math.sqrt(sum(np.square(hubs_new)))\n",
    "\n",
    "#     authdiff = np.abs(auth_new - auth)\n",
    "#     hubdiff = np.abs(hubs_new - hubs)\n",
    "    \n",
    "#     print(i , end = \", \")\n",
    "#     if (authdiff > eps).any():\n",
    "#         auth = auth_new\n",
    "#         hubs = hubs_new\n",
    "#         continue\n",
    "#     else:    \n",
    "#         print (\"found\")\n",
    "#         break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id in auth.argsort()[::-1][0:500]:\n",
    "#     print(base_rev_set_mapping[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id in hubs.argsort()[::-1][0:500]:\n",
    "#     print(base_rev_set_mapping[id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
