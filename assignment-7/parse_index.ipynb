{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from elasticsearch import Elasticsearch\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = set(w.lower() for w in nltk.corpus.words.words())\n",
    "es = Elasticsearch()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = glob(\"trec07p/data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_ham = {}\n",
    "with open(\"trec07p/full/index\") as file:\n",
    "    text = file.read().split(\"\\n\")[:-1]\n",
    "    for line in text:\n",
    "        line = line.split(\" ..\")\n",
    "        if line[0] == \"spam\":\n",
    "            spam_ham[line[1].split(\"/\")[2]] = True\n",
    "        else:\n",
    "            spam_ham[line[1].split(\"/\")[2]] = False\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75419"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_perct = len(data_paths) * 20 / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_partition = {}\n",
    "test_set = random.sample(range(0, len(data_paths)), int(twenty_perct))\n",
    "for i in range(len(data_paths)):\n",
    "    if i in test_set:\n",
    "        data_partition[data_paths[i]] = False\n",
    "    else:\n",
    "        data_partition[data_paths[i]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = {\n",
    "   \"settings\": {\n",
    "      \"analysis\": {\n",
    "         \"analyzer\": {\n",
    "            \"evolutionAnalyzer\": {\n",
    "               \"tokenizer\": \"standard\",\n",
    "               \"filter\": [\n",
    "                  \"standard\",\n",
    "                  \"lowercase\",\n",
    "                  \"custom_shingle\"\n",
    "               ]\n",
    "            }\n",
    "         },\n",
    "         \"filter\": {\n",
    "            \"custom_shingle\": {\n",
    "               \"type\": \"shingle\",\n",
    "               \"min_shingle_size\": \"2\",\n",
    "               \"max_shingle_size\": \"4\",\n",
    "               \"filler_token\": \"\",\n",
    "               \"output_unigrams\": True\n",
    "            }\n",
    "         }\n",
    "      }\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"doc\": {\n",
    "         \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "               \"analyzer\": \"evolutionAnalyzer\",\n",
    "               \"search_analyzer\": \"standard\",\n",
    "               \"term_vector\": \"yes\"\n",
    "            },\n",
    "             \"name\": {\n",
    "                \"type\": \"keyword\",\n",
    "             },\n",
    "             \"train\": {\n",
    "                 \"type\" : \"text\"\n",
    "             },\n",
    "             \"spam\": {\n",
    "                 \"type\" : \"text\"\n",
    "             }\n",
    "         }\n",
    "      }\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es.indices.create(index = 'spam_ham_2', body = request_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for path in data_paths:\n",
    "    with open(path, encoding=\"utf8\", errors='ignore') as file:\n",
    "        index = 0\n",
    "        fulltext = file.read().lower()\n",
    "\n",
    "        if(fulltext.find(\"content-type: text/html;\") != -1):\n",
    "            soup = BeautifulSoup(fulltext, 'html.parser')\n",
    "            fulltext = soup.get_text().replace(\"<\", \"\").replace(\"/>\",\"\")\n",
    "\n",
    "        lines = fulltext.split(\"\\n\")[1:]\n",
    "        length = len(lines)\n",
    "        text = []\n",
    "\n",
    "        ignore_lines = [\"Return-Path: \", \"Date: \",  \"From: \", \"To: \", \"Message-ID: \", \"References: \",\n",
    "                       \"Content-Disposition: \", \"Mime-Version: \", \"In-Reply-To: \", \"X-Virus-Scanned: \",\n",
    "                       \"X-Virus-Status: \", \"User-Agent: \", \"X-Chzlrs: \", \"X-BeenThere: \", \"X-Mailman-Version: \",\n",
    "                       \"Precedence: \", \"List-Id: \", \"List-Unsubscribe: \", \"List-Archive: \", \"List-Post: \",\n",
    "                       \"List-Help: \", \"List-Subscribe: \", \"Content-Type: \", \"Content-Transfer-Encoding: \",\n",
    "                       \"Sender: \", \"Errors-To: \", \"X-Spam-Checker-Version: \", \"X-Spam-Level: \",\n",
    "                       \"X-Spam-Status: \", \"X-Mailer: \", \"X-Priority: \", \"X-Spam: \", \"X-Miltered: \", \"X-UUID: \",\n",
    "                       \"Status: \", \"X-Miltered: \", \"Content-Length: \", \"Lines: \", \"X-VirtualServer: \",\n",
    "                       \"X-VirtualServerGroup: \", \"X-Destination-ID: \", \"X-MailingID: \", \"X-SMFBL: \",\n",
    "                       \"X-SMHeaderMap: \", \"Lines: \", \"X-Original-To: \", \"Delivered-To: \", \"x-mimeole: \",\n",
    "                       \"thread-index: \", \"svn commit: \", \"received: \", \"received-spf: \", \"x-spam-check-by: \",\n",
    "                       \"x-posted-by: \", \"broadcastjobid: \", \"content-class: \", \"importance: \", \n",
    "                       \"priority: \", \"x-originalarrivaltime: \", \"x-keywords: \"]\n",
    "\n",
    "        replace_words = [\"subject: \", \"reply-to: \", \"re: \", \"thread-topic: \", \"ref\"]\n",
    "        while index < length:\n",
    "            line = lines[index]\n",
    "\n",
    "            if len(line) >= 9 :\n",
    "\n",
    "                if line[0:9].lower() == \"Received:\".lower():\n",
    "                    index = index + 2\n",
    "                    line = lines[index]\n",
    "                    \n",
    "                \n",
    "                if len(line) > 0 and line[0] == \"\\t\":\n",
    "                    index = index + 1\n",
    "                    continue\n",
    "\n",
    "            ig = False\n",
    "            for word in ignore_lines:\n",
    "                if line.lower().find(word.lower()) != -1:\n",
    "                    ig = True\n",
    "                    break\n",
    "\n",
    "            if ig == True:\n",
    "                index = index + 1\n",
    "                continue\n",
    "\n",
    "            line = line.lower()\n",
    "            for word in replace_words:\n",
    "                line = line.replace(word.lower(), \"\")\n",
    "\n",
    "            line = line.replace(\"$\",\"dollar\").replace(\"%\",\"percentage\")\n",
    "            text.append(line)\n",
    "            index = index + 1\n",
    "    fulltext = \"\\n\".join(text)\n",
    "    fulltext_token = tokenizer.tokenize(fulltext)\n",
    "\n",
    "\n",
    "\n",
    "    final_text = []\n",
    "    for word in fulltext_token:\n",
    "        if word in english_words:\n",
    "            final_text.append(word)\n",
    "    \n",
    "    file_name = path.split(\"/\")[2]\n",
    "    \n",
    "    _body = {\n",
    "        \"text\" : file_name,\n",
    "        \"name\" : path,\n",
    "        \"train\" : str(data_partition[path]),\n",
    "        \"spam\" : str(spam_ham[file_name])\n",
    "    }\n",
    "   \n",
    "    es.index(index='spam_ham', doc_type='doc', id=file_name, body=_body)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
